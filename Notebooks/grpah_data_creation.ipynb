{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one grpahs dataset contain:\n",
    "1. 12 set of node features (num_nodes, node_features)*12\n",
    "2. static edge indexes (2, nodes)\n",
    "3. 12 set of edge attributes (num_edges , edge feature dim)*12\n",
    "4. 12 set of time attributes (1 , num_nodes)*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset : bus dataset\n",
    "dataset=pd.read_csv('dataset_final.csv',parse_dates=[\"datetime\"]).drop('Unnamed: 0',axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset : stop dataset\n",
    "stops = pd.read_csv('../bus_stops_rio_de_janeiro.csv',delimiter=\"|\")\n",
    "stops = stops[stops['line_number'].str.match(r'^-?\\d+(\\.\\d+)?$', na=False)]\n",
    "stops['line_number'] = stops['line_number'].astype(float)\n",
    "stops = stops[stops[\"line_number\"].isin(dataset[\"line\"].unique())]\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops[\"num_routes_at_stop\"] = stops.groupby(\"stop_name\")[\"line_number\"].transform(\"nunique\")\n",
    "stops[\"is_shared_stop\"] = stops[\"num_routes_at_stop\"] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out few routes\n",
    "stops = stops[stops['line_number'].isin([847,935 ,334])]\n",
    "dataset = dataset[dataset['line'].isin([847,935 ,334])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add shared stop feature dataset\n",
    "stop_shared_dict = stops[['stop_name', 'is_shared_stop']].drop_duplicates().set_index('stop_name')['is_shared_stop'].to_dict()\n",
    "dataset['is_shared_stop'] = dataset['nearest_stop_name'].map(stop_shared_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features : stop dataset\n",
    "stops[\"prev_latitude\"] = stops.groupby(\"line_number\")[\"stop_latitude\"].shift(1)\n",
    "stops[\"prev_longitude\"] = stops.groupby(\"line_number\")[\"stop_longitude\"].shift(1)\n",
    "\n",
    "def haversine(row):\n",
    "    if pd.isna(row[\"prev_latitude\"]) or pd.isna(row[\"prev_longitude\"]):\n",
    "        return 0\n",
    "    return geodesic((row[\"prev_latitude\"], row[\"prev_longitude\"]), (row[\"stop_latitude\"], row[\"stop_longitude\"])).meters\n",
    "stops[\"distance_from_previous_stop\"] = stops.apply(haversine, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features : bus dataset\n",
    "dataset[\"hour\"] = dataset[\"datetime\"].dt.hour\n",
    "dataset[\"minute\"] = dataset[\"datetime\"].dt.minute\n",
    "dataset[\"is_weekday\"] = dataset[\"datetime\"].dt.weekday < 5 \n",
    "dataset[\"is_weekday\"] = dataset[\"is_weekday\"].astype(int)\n",
    "dataset[\"travel_time\"] = dataset[\"distance_to_next_stop\"] / dataset[\"speed\"].replace(0, np.nan)\n",
    "dataset[\"travel_time\"] = dataset[\"travel_time\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge features: distance, route_multiplicity (1) static\n",
    "# node features: distance_to_next_stop, is_at_stop, speed , hour , minute , is_shared_stop (1) dynamic\n",
    "# time feature: is_weekday (1) dynamic\n",
    "# target: travel_time (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edge dataset\n",
    "edges = []\n",
    "for line in stops['line_number'].unique():\n",
    "    line_data = stops[stops['line_number'] == line]\n",
    "    \n",
    "    for i in range(len(line_data) - 1):\n",
    "        stop1 = line_data.iloc[i]\n",
    "        stop2 = line_data.iloc[i + 1]\n",
    "        distance = stop2['distance_from_previous_stop']\n",
    "        # (start stop, end stop, distance)\n",
    "        edge = {\n",
    "            'start_stop': stop1['stop_name'],\n",
    "            'end_stop': stop2['stop_name'],\n",
    "            'distance': distance,\n",
    "            'route_multiplicity': None\n",
    "        }\n",
    "        edges.append(edge)\n",
    "edges_df = pd.DataFrame(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add route_multiplicity\n",
    "shared_stops = dataset.set_index(\"nearest_stop_name\")[\"is_shared_stop\"].to_dict()\n",
    "edges_df[\"route_multiplicity\"] = edges_df.apply(\n",
    "    lambda row: shared_stops.get(row[\"start_stop\"], False) and shared_stops.get(row[\"end_stop\"], False),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode node datset\n",
    "encode_columns = [\"order\", \"nearest_stop_name\", \"is_at_stop\", \"is_shared_stop\"]\n",
    "encoders = {}\n",
    "\n",
    "for col in encode_columns:\n",
    "    le = LabelEncoder()\n",
    "    dataset[col] = le.fit_transform(dataset[col])\n",
    "    encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode edge datset\n",
    "encoded_edge_df =edges_df.copy()\n",
    "encode_columns = [\"start_stop\", \"end_stop\"]\n",
    "for col in encode_columns:\n",
    "    if col in encoded_edge_df:\n",
    "        encoded_edge_df[col] = encoded_edge_df[col].map(lambda x: encoders['nearest_stop_name'].transform([x])[0] if x in encoders['nearest_stop_name'].classes_ else -1)\n",
    "\n",
    "binary_mapping = {True: 1, False: 0}\n",
    "encoded_edge_df[\"route_multiplicity\"] = encoded_edge_df[\"route_multiplicity\"].map(binary_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_edge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edge indices\n",
    "encoded_edge_df=encoded_edge_df.drop_duplicates(subset=['start_stop', 'end_stop'])\n",
    "edge_indices = torch.tensor([\n",
    "    encoded_edge_df[\"start_stop\"].values, \n",
    "    encoded_edge_df[\"end_stop\"].values\n",
    "], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edge features\n",
    "edge_features = encoded_edge_df[['distance', 'route_multiplicity']].values\n",
    "edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create node features\n",
    "node_dataset = dataset.copy().sort_values(by=['datetime']).drop(['date','time','latitude','longitude','datetime','is_weekday','travel_time'],axis=1)\n",
    "node_features = node_dataset.groupby('nearest_stop_name').apply(lambda x: x.drop(columns=['nearest_stop_name']).values.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_snapshots = sum(len(v) for v in node_features.values())\n",
    "\n",
    "node_queues = {node: list(features) for node, features in node_features.items()}\n",
    "last_known_features = {node: node_queues[node][0] for node in node_queues}  # Initialize with first feature\n",
    "\n",
    "node_snapshots = []\n",
    "\n",
    "while any(node_queues.values()):  \n",
    "    snapshot_features = []\n",
    "    \n",
    "    for node in node_queues:\n",
    "        if node_queues[node]:  \n",
    "            last_known_features[node] = node_queues[node].pop(0)  \n",
    "        snapshot_features.append(last_known_features[node])  \n",
    "    \n",
    "    node_snapshots.append(snapshot_features)  \n",
    "\n",
    "node_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time features\n",
    "time_dataset = dataset.copy().sort_values(by=['datetime'])[['nearest_stop_name','is_weekday']]\n",
    "time_features = time_dataset.groupby('nearest_stop_name').apply(lambda x: x.drop(columns=['nearest_stop_name']).values.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = {key: [item[0] for item in value] for key, value in time_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_queues = {node: list(features) for node, features in time_features.items()}\n",
    "last_known_features = {node: time_queues[node][0] for node in time_queues}  #\n",
    "\n",
    "time_snapshots = []\n",
    "\n",
    "while any(time_queues.values()):  \n",
    "    snapshot_features = []\n",
    "    \n",
    "    for node in time_queues:\n",
    "        if time_queues[node]:  \n",
    "            last_known_features[node] = time_queues[node].pop(0) \n",
    "        snapshot_features.append(last_known_features[node])  \n",
    "    \n",
    "    time_snapshots.append(snapshot_features)\n",
    "\n",
    "time_snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target feature\n",
    "target_dataset = dataset.copy().sort_values(by=['datetime'])[['nearest_stop_name','travel_time']]\n",
    "target_features = target_dataset.groupby('nearest_stop_name').apply(lambda x: x.drop(columns=['nearest_stop_name']).values.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = {key: [item[0] for item in value] for key, value in target_features.items()}\n",
    "target_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(target_features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_snapshots(12, num_nodes , node_feature_dim), \n",
    "# edge_indices(2, edge_count) , \n",
    "# edge_features(12 ,edge_count , edge_feature_dim) , \n",
    "# time_snapshots(12, num_nodes)  , | \n",
    "# target_features(num_nodes, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_nodes = len(target_features)\n",
    "max_length = max(len(v) for v in target_features.values())  # Longest queue\n",
    "\n",
    "# Ensure all lists are padded with their last value if shorter\n",
    "for node in target_features:\n",
    "    while len(target_features[node]) < max_length:\n",
    "        target_features[node].append(target_features[node][-1])  \n",
    "\n",
    "target_batches = {node: [] for node in target_features}\n",
    "num_batches = max_length - batch_size + 1\n",
    "\n",
    "for node, values in target_features.items():\n",
    "    for i in range(num_batches):\n",
    "        target_batches[node].append(values[i : i + batch_size])\n",
    "\n",
    "target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_batches.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_snapshots = len(time_snapshots)\n",
    "\n",
    "time_snapshot_batches = []\n",
    "\n",
    "for i in range(num_snapshots - batch_size + 1):  \n",
    "    time_snapshot_batches.append(time_snapshots[i : i + batch_size])\n",
    "time_snapshot_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features_batches = [edge_features]*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_snapshots = len(node_snapshots)\n",
    "\n",
    "node_snapshot_batches = []\n",
    "\n",
    "for i in range(num_snapshots - batch_size + 1):  \n",
    "    node_snapshot_batches.append(node_snapshots[i : i + batch_size])\n",
    "node_snapshot_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(node_snapshot_batches[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(target_batches[0]))\n",
    "display(len(node_snapshot_batches))\n",
    "display(len(edge_features_batches))\n",
    "display(len(time_snapshot_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seq = []\n",
    "for i in range(len(node_snapshot_batches)):\n",
    "    seq = [torch.tensor(node_snapshot_batches[i]), torch.tensor(edge_indices), torch.tensor(edge_features_batches,dtype=torch.float32), torch.tensor(time_snapshot_batches[i])]\n",
    "    graph_seq.append(seq)\n",
    "graph_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save input datas\n",
    "torch.save(graph_seq, \"../train_data/rio_data/rio_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tensor\n",
    "loaded_tensor = torch.load(\"../train_data/rio_data/rio_data.pth\")\n",
    "loaded_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(target_batches.keys())\n",
    "num_snapshots = len(next(iter(target_batches.values())))  # 4300\n",
    "\n",
    "target_snapshot_batches = []\n",
    "for i in range(num_snapshots):  \n",
    "    batch = [target_batches[node][i] for node in nodes]  # Collect i-th snapshot from each node\n",
    "    target_snapshot_batches.append(batch)\n",
    "\n",
    "snapshot_batches_tensor = torch.tensor(target_snapshot_batches, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save target datas\n",
    "torch.save(snapshot_batches_tensor, \"../train_data/rio_data/rio_data_target.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_batches_tensor[4299]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
